# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-23 11:24+0800\n"
"PO-Revision-Date: 2026-01-22 17:05+0800\n"
"Last-Translator: Gemini\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/tutorials/Qwen3-235B-A22B.md:1
msgid "Qwen3-235B-A22B"
msgstr "Qwen3-235B-A22B"

#: ../../source/tutorials/Qwen3-235B-A22B.md:3
msgid "Introduction"
msgstr "简介"

#: ../../source/tutorials/Qwen3-235B-A22B.md:5
msgid ""
"Qwen3 is the latest generation of large language models in Qwen series, "
"offering a comprehensive suite of dense and mixture-of-experts (MoE) "
"models. Built upon extensive training, Qwen3 delivers groundbreaking "
"advancements in reasoning, instruction-following, agent capabilities, and"
" multilingual support."
msgstr ""
"Qwen3 是通义千问系列最新一代的大语言模型，提供了一套完整的稠密（dense）和混合专家（MoE）模型。基于海量的训练数据，Qwen3 "
"在推理、指令遵循、智能体能力和多语言支持方面取得了突破性的进展。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:7
msgid ""
"This document will show the main verification steps of the model, "
"including supported features, feature configuration, environment "
"preparation, single-node and multi-node deployment, accuracy and "
"performance evaluation."
msgstr "本文档将展示该模型的主要验证步骤，包括支持的功能、功能配置、环境准备、单节点及多节点部署、精度和性能评估。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:9
msgid "The `Qwen3-235B-A22B` model is first supported in `vllm-ascend:v0.8.4rc2`."
msgstr "`Qwen3-235B-A22B` 模型最早在 `vllm-ascend:v0.8.4rc2` 版本中得到支持。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:11
msgid "Supported Features"
msgstr "支持的功能"

#: ../../source/tutorials/Qwen3-235B-A22B.md:13
msgid ""
"Refer to [supported "
"features](../user_guide/support_matrix/supported_models.md) to get the "
"model's supported feature matrix."
msgstr ""
"请参考 [支持功能列表](../user_guide/support_matrix/supported_models.md) "
"以获取模型支持的功能矩阵。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:15
msgid ""
"Refer to [feature guide](../user_guide/feature_guide/index.md) to get the"
" feature's configuration."
msgstr "请参考 [功能指南](../user_guide/feature_guide/index.md) 获取各项功能的配置。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:17
msgid "Environment Preparation"
msgstr "环境准备"

#: ../../source/tutorials/Qwen3-235B-A22B.md:19
msgid "Model Weight"
msgstr "模型权重"

#: ../../source/tutorials/Qwen3-235B-A22B.md:21
msgid ""
"`Qwen3-235B-A22B`(BF16 version): require 1 Atlas 800 A3 (64G × 16) node， "
"1 Atlas 800 A2 (64G × 8) node or 2 Atlas 800 A2（32G * 8）nodes. [Download "
"model weight](https://modelers.cn/models/Modelers_Park/Qwen3-235B-A22B)"
msgstr ""
"`Qwen3-235B-A22B`（BF16 版本）：需要 1 个 Atlas 800 A3（64G * 16）节点、1 个 Atlas 800 "
"A2（64G * 8）节点或 2 个 Atlas 800 A2（32G * "
"8）节点。[下载模型权重](https://modelers.cn/models/Modelers_Park/Qwen3-235B-A22B)"

#: ../../source/tutorials/Qwen3-235B-A22B.md:22
msgid ""
"`Qwen3-235B-A22B-w8a8`(Quantized version): require 1 Atlas 800 A3 (64G × "
"16) node or 1 Atlas 800 A2 (64G × 8) node or 2 Atlas 800 A2（32G * "
"8）nodes. [Download model weight](https://modelscope.cn/models/vllm-"
"ascend/Qwen3-235B-A22B-W8A8)"
msgstr ""
"`Qwen3-235B-A22B-w8a8`（量化版本）：需要 1 个 Atlas 800 A3（64G * 16）节点、1 个 Atlas "
"800 A2（64G * 8）节点或 2 个 Atlas 800 A2（32G * "
"8）节点。[下载模型权重](https://modelscope.cn/models/vllm-ascend/Qwen3-235B-A22B-"
"W8A8)"

#: ../../source/tutorials/Qwen3-235B-A22B.md:24
msgid ""
"It is recommended to download the model weight to the shared directory of"
" multiple nodes, such as `/root/.cache/`"
msgstr "建议将模型权重下载到多节点共享目录中，例如 `/root/.cache/`。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:26
msgid "Verify Multi-node Communication(Optional)"
msgstr "验证多节点通信（可选）"

#: ../../source/tutorials/Qwen3-235B-A22B.md:28
msgid ""
"If you want to deploy multi-node environment, you need to verify multi-"
"node communication according to [verify multi-node communication "
"environment](../installation.md#verify-multi-node-communication)."
msgstr ""
"如果您想部署多节点环境，请参考 [验证多节点通信环境](../installation.md#verify-multi-node-"
"communication) 进行验证。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:30
msgid "Installation"
msgstr "安装"

#: ../../source/tutorials/Qwen3-235B-A22B.md
msgid "Use docker image"
msgstr "使用 Docker 镜像"

#: ../../source/tutorials/Qwen3-235B-A22B.md:36
msgid ""
"For example, using images `quay.io/ascend/vllm-ascend:v0.11.0rc2`(for "
"Atlas 800 A2) and `quay.io/ascend/vllm-ascend:v0.11.0rc2-a3`(for Atlas "
"800 A3)."
msgstr ""
"例如，使用镜像 `quay.io/ascend/vllm-ascend:v0.11.0rc2`（适用于 Atlas 800 A2）或 "
"`quay.io/ascend/vllm-ascend:v0.11.0rc2-a3`（适用于 Atlas 800 A3）。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:38
msgid ""
"Select an image based on your machine type and start the docker image on "
"your node, refer to [using docker](../installation.md#set-up-using-"
"docker)."
msgstr ""
"根据您的机器类型选择镜像并在节点上启动 Docker 镜像，详见 [使用 Docker](../installation.md#set-up-"
"using-docker)。"

#: ../../source/tutorials/Qwen3-235B-A22B.md
msgid "Build from source"
msgstr "从源码构建"

#: ../../source/tutorials/Qwen3-235B-A22B.md:78
msgid "You can build all from source."
msgstr "您可以直接从源码进行构建。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:80
msgid ""
"Install `vllm-ascend`, refer to [set up using python](../installation.md"
"#set-up-using-python)."
msgstr ""
"安装 `vllm-ascend`，请参考 [使用 Python 安装](../installation.md#set-up-using-"
"python)。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:84
msgid ""
"If you want to deploy multi-node environment, you need to set up "
"environment on each node."
msgstr "如果您要部署多节点环境，需要在每个节点上分别进行环境配置。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:86
msgid "Deployment"
msgstr "部署"

#: ../../source/tutorials/Qwen3-235B-A22B.md:88
msgid "Single-node Deployment"
msgstr "单节点部署"

#: ../../source/tutorials/Qwen3-235B-A22B.md:90
msgid ""
"`Qwen3-235B-A22B` and `Qwen3-235B-A22B-w8a8` can both be deployed on 1 "
"Atlas 800 A3（64G*16）、 1 Atlas 800 A2（64G*8）. Quantized version need to "
"start with parameter `--quantization ascend`."
msgstr ""
"`Qwen3-235B-A22B` 和 `Qwen3-235B-A22B-w8a8` 均可部署在 1 台 Atlas 800 "
"A3（64G*16）或 1 台 Atlas 800 A2（64G*8）上。量化版本启动时需添加参数 `--quantization "
"ascend`。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:93
msgid "Run the following script to execute online 128k inference."
msgstr "运行以下脚本以执行 128k 上下文的在线推理。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:126
msgid "**Notice:**"
msgstr "**注意：**"

#: ../../source/tutorials/Qwen3-235B-A22B.md:127
msgid ""
"[Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B#processing-"
"long-texts) originally only supports 40960 "
"context(max_position_embeddings). If you want to use it and its related "
"quantization weights to run long seqs (such as 128k context), it is "
"required to use yarn rope-scaling technique."
msgstr ""
"[Qwen3-235B-A22B](https://huggingface.co/Qwen/Qwen3-235B-A22B#processing-"
"long-texts) 原生仅支持 40960 "
"上下文（max_position_embeddings）。如果您希望使用该模型及其相关量化权重运行长序列（如 128k 上下文），必须使用 "
"yarn rope-scaling 技术。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:128
#, python-brace-format
msgid ""
"For vLLM version same as or new than `v0.12.0`, use parameter: `--hf-"
"overrides '{\"rope_parameters\": "
"{\"rope_type\":\"yarn\",\"rope_theta\":1000000,\"factor\":4,\"original_max_position_embeddings\":32768}}'"
" \\`."
msgstr ""
"对于等于或高于 `v0.12.0` 的 vLLM 版本，请使用参数：`--hf-overrides '{\"rope_parameters\": "
"{\"rope_type\":\"yarn\",\"rope_theta\":1000000,\"factor\":4,\"original_max_position_embeddings\":32768}}'"
" \\`。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:129
#, python-brace-format
msgid ""
"For vllm version below `v0.12.0`, use parameter: `--rope_scaling "
"'{\"rope_type\":\"yarn\",\"factor\":4,\"original_max_position_embeddings\":32768}'"
" \\`. If you are using weights like [Qwen3-235B-A22B-"
"Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)"
" which originally supports long contexts, there is no need to add this "
"parameter."
msgstr ""
"对于低于 `v0.12.0` 的 vLLM 版本，请使用参数：`--rope_scaling "
"'{\"rope_type\":\"yarn\",\"factor\":4,\"original_max_position_embeddings\":32768}'"
" \\`。如果您使用的是类似 [Qwen3-235B-A22B-"
"Instruct-2507](https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507)"
" 这种原生支持长上下文的权重，则无需添加此参数。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:132
msgid "The parameters are explained as follows:"
msgstr "参数说明如下："

#: ../../source/tutorials/Qwen3-235B-A22B.md:133
msgid ""
"`--data-parallel-size` 1 and `--tensor-parallel-size` 8 are common "
"settings for data parallelism (DP) and tensor parallelism (TP) sizes."
msgstr ""
"`--data-parallel-size` 1 和 `--tensor-parallel-size` 8 是数据并行 (DP) 和张量并行 "
"(TP) 大小的常用设置。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:134
msgid ""
"`--max-model-len` represents the context length, which is the maximum "
"value of the input plus output for a single request."
msgstr "`--max-model-len` 代表上下文长度，即单次请求输入加输出的最大 token 数。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:135
msgid ""
"`--max-num-seqs` indicates the maximum number of requests that each DP "
"group is allowed to process. If the number of requests sent to the "
"service exceeds this limit, the excess requests will remain in a waiting "
"state and will not be scheduled. Note that the time spent in the waiting "
"state is also counted in metrics such as TTFT and TPOT. Therefore, when "
"testing performance, it is generally recommended that `--max-num-seqs` * "
"`--data-parallel-size` >= the actual total concurrency."
msgstr ""
"`--max-num-seqs` 表示每个 DP "
"组允许处理的最大请求数。如果发送给服务的请求数超过此限制，多出的请求将处于等待状态且不会被调度。请注意，等待状态的时间也会被计入 TTFT 和 "
"TPOT 等指标。因此，在测试性能时，通常建议使 `--max-num-seqs` * `--data-parallel-size` "
"大于或等于实际总并发量。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:136
msgid ""
"`--max-num-batched-tokens` represents the maximum number of tokens that "
"the model can process in a single step. Currently, vLLM v1 scheduling "
"enables ChunkPrefill/SplitFuse by default, which means:"
msgstr ""
"`--max-num-batched-tokens` 代表模型在单步中可以处理的最大 token 数。目前，vLLM v1 调度默认开启了 "
"ChunkPrefill/SplitFuse，这意味着："

#: ../../source/tutorials/Qwen3-235B-A22B.md:137
msgid ""
"(1) If the input length of a request is greater than `--max-num-batched-"
"tokens`, it will be divided into multiple rounds of computation according"
" to `--max-num-batched-tokens`;"
msgstr "(1) 如果请求的输入长度大于 `--max-num-batched-tokens`，它将根据该值被划分为多轮计算；"

#: ../../source/tutorials/Qwen3-235B-A22B.md:138
msgid ""
"(2) Decode requests are prioritized for scheduling, and prefill requests "
"are scheduled only if there is available capacity."
msgstr "(2) 解码（Decode）请求会被优先调度，只有在有可用容量时才会调度预填充（Prefill）请求。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:139
msgid ""
"Generally, if `--max-num-batched-tokens` is set to a larger value, the "
"overall latency will be lower, but the pressure on GPU memory (activation"
" value usage) will be greater."
msgstr "通常情况下，`--max-num-batched-tokens` 设置得越大，整体延迟越低，但对显存（激活值占用）的压力也越大。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:140
msgid ""
"`--gpu-memory-utilization` represents the proportion of HBM that vLLM "
"will use for actual inference. Its essential function is to calculate the"
" available kv_cache size. During the warm-up phase (referred to as "
"profile run in vLLM), vLLM records the peak GPU memory usage during an "
"inference process with an input size of `--max-num-batched-tokens`. The "
"available kv_cache size is then calculated as: `--gpu-memory-utilization`"
" * HBM size - peak GPU memory usage. Therefore, the larger the value of "
"`--gpu-memory-utilization`, the more kv_cache can be used. However, since"
" the GPU memory usage during the warm-up phase may differ from that "
"during actual inference (e.g., due to uneven EP load), setting `--gpu-"
"memory-utilization` too high may lead to OOM (Out of Memory) issues "
"during actual inference. The default value is `0.9`."
msgstr ""
"`--gpu-memory-utilization` 代表 vLLM 用于实际推理的显存比例。其核心功能是计算可用的 kv_cache "
"大小。在预热阶段（vLLM 中称为 profile run），vLLM 会记录输入大小为 `--max-num-batched-tokens` "
"时推理过程的峰值显存占用。可用的 kv_cache 大小计算公式为： `--gpu-memory-utilization` * 显存总量 - "
"峰值显存占用。因此，该值设置得越大，可用的 kv_cache 就越多。但由于预热阶段的显存占用可能与实际推理（例如 EP "
"负载不均）有所不同，该值设置过高可能会导致实际推理中出现 OOM（显存溢出）。默认值为 `0.9`。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:141
msgid ""
"`--enable-expert-parallel` indicates that EP is enabled. Note that vLLM "
"does not support a mixed approach of ETP and EP; that is, MoE can either "
"use pure EP or pure TP."
msgstr ""
"`--enable-expert-parallel` 表示开启专家并行 (EP)。注意 vLLM 目前不支持 ETP 和 EP "
"混合模式；也就是说，MoE 只能使用纯 EP 或纯 TP。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:142
msgid ""
"`--no-enable-prefix-caching` indicates that prefix caching is disabled. "
"To enable it, remove this option."
msgstr "`--no-enable-prefix-caching` 表示禁用前缀缓存。若要开启，请移除此选项。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:143
msgid ""
"`--quantization` \"ascend\" indicates that quantization is used. To "
"disable quantization, remove this option."
msgstr "`--quantization` \"ascend\" 表示使用量化。若要禁用量化，请移除此选项。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:144
msgid ""
"`--compilation-config` contains configurations related to the aclgraph "
"graph mode. The most significant configurations are \"cudagraph_mode\" "
"and \"cudagraph_capture_sizes\", which have the following meanings: "
"\"cudagraph_mode\": represents the specific graph mode. Currently, "
"\"PIECEWISE\" and \"FULL_DECODE_ONLY\" are supported. The graph mode is "
"mainly used to reduce the cost of operator dispatch. Currently, "
"\"FULL_DECODE_ONLY\" is recommended."
msgstr ""
"`--compilation-config` 包含与 aclgraph 图模式相关的配置。最重要的配置是 \"cudagraph_mode\" 和"
" \"cudagraph_capture_sizes\"，含义如下： \"cudagraph_mode\"：表示具体的图模式。目前支持 "
"\"PIECEWISE\" 和 \"FULL_DECODE_ONLY\"。图模式主要用于减少算子调度开销。目前推荐使用 "
"\"FULL_DECODE_ONLY\"。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:146
msgid ""
"\"cudagraph_capture_sizes\": represents different levels of graph modes. "
"The default value is [1, 2, 4, 8, 16, 24, 32, 40,..., `--max-num-seqs`]. "
"In the graph mode, the input for graphs at different levels is fixed, and"
" inputs between levels are automatically padded to the next level. "
"Currently, the default setting is recommended. Only in some scenarios is "
"it necessary to set this separately to achieve optimal performance."
msgstr ""
"\"cudagraph_capture_sizes\"：代表不同层级的图模式捕获大小。默认值为 [1, 2, 4, 8, 16, 24, 32, "
"40,..., `--max-num-"
"seqs`]。在图模式中，各层级图的输入是固定的，层级之间的输入会自动填充到下一层级。目前建议使用默认设置，仅在特定场景下才需要单独设置以达到最佳性能。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:147
msgid ""
"`export VLLM_ASCEND_ENABLE_FLASHCOMM1=1` indicates that Flashcomm1 "
"optimization is enabled. Currently, this optimization is only supported "
"for MoE in scenarios where tp_size > 1."
msgstr ""
"`export VLLM_ASCEND_ENABLE_FLASHCOMM1=1` 表示开启 Flashcomm1 优化。目前，该优化仅在 "
"tp_size > 1 的 MoE 场景下支持。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:149
msgid "Multi-node Deployment with MP (Recommended)"
msgstr "使用 MP 进行多节点部署（推荐）"

#: ../../source/tutorials/Qwen3-235B-A22B.md:150
msgid ""
"Assume you have Atlas 800 A3 (64G*16) nodes (or 2 * A2), and want to "
"deploy the `Qwen3-VL-235B-A22B-Instruct` model across multiple nodes."
msgstr ""
"假设您拥有 Atlas 800 A3（64G*16）节点（或 2 台 A2），并希望在多节点间部署 `Qwen3-VL-235B-A22B-"
"Instruct` 模型。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:152
msgid "Node 0"
msgstr "节点 0"

#: ../../source/tutorials/Qwen3-235B-A22B.md:194
msgid "Node1"
msgstr "节点 1"

#: ../../source/tutorials/Qwen3-235B-A22B.md:240
msgid ""
"If the service starts successfully, the following information will be "
"displayed on node 0:"
msgstr "如果服务成功启动，节点 0 上将显示以下信息："

#: ../../source/tutorials/Qwen3-235B-A22B.md:251
msgid "Multi-node Deployment with Ray"
msgstr "使用 Ray 进行多节点部署"

#: ../../source/tutorials/Qwen3-235B-A22B.md:253
msgid "refer to [Ray Distributed (Qwen/Qwen3-235B-A22B)](./ray.md)."
msgstr "请参考 [Ray 分布式部署 (Qwen/Qwen3-235B-A22B)](./ray.md)。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:255
msgid "Prefill-Decode Disaggregation"
msgstr "预填充-解码分离（PD 分离）"

#: ../../source/tutorials/Qwen3-235B-A22B.md:257
msgid ""
"refer to [Prefill-Decode Disaggregation Mooncake Verification "
"(Qwen)](./pd_disaggregation_mooncake_multi_node.md)"
msgstr ""
"请参考 [基于 Mooncake 的预填充-解码分离验证 "
"(Qwen)](./pd_disaggregation_mooncake_multi_node.md)"

#: ../../source/tutorials/Qwen3-235B-A22B.md:259
msgid "Functional Verification"
msgstr "功能验证"

#: ../../source/tutorials/Qwen3-235B-A22B.md:261
msgid "Once your server is started, you can query the model with input prompts:"
msgstr "服务启动后，您可以使用输入提示词查询模型："

#: ../../source/tutorials/Qwen3-235B-A22B.md:274
msgid "Accuracy Evaluation"
msgstr "精度评估"

#: ../../source/tutorials/Qwen3-235B-A22B.md:276
msgid "Here are two accuracy evaluation methods."
msgstr "以下是两种精度评估方法。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:278
#: ../../source/tutorials/Qwen3-235B-A22B.md:290
msgid "Using AISBench"
msgstr "使用 AISBench"

#: ../../source/tutorials/Qwen3-235B-A22B.md:280
msgid ""
"Refer to [Using "
"AISBench](../developer_guide/evaluation/using_ais_bench.md) for details."
msgstr "详见 [使用 AISBench](../developer_guide/evaluation/using_ais_bench.md)。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:282
msgid ""
"After execution, you can get the result, here is the result of `Qwen3"
"-235B-A22B-w8a8` in `vllm-ascend:0.11.0rc0` for reference only."
msgstr "执行后即可获取结果，以下是 `vllm-ascend:0.11.0rc0` 下 `Qwen3-235B-A22B-w8a8` 的结果，仅供参考。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "dataset"
msgstr "数据集"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "version"
msgstr "版本"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "metric"
msgstr "指标"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "mode"
msgstr "模式"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "vllm-api-general-chat"
msgstr "vllm-api-general-chat"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "cevaldataset"
msgstr "cevaldataset"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "-"
msgstr "-"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "accuracy"
msgstr "准确率"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "gen"
msgstr "生成"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "91.16"
msgstr "91.16"

#: ../../source/tutorials/Qwen3-235B-A22B.md:288
msgid "Performance"
msgstr "性能指标"

#: ../../source/tutorials/Qwen3-235B-A22B.md:292
msgid ""
"Refer to [Using AISBench for performance "
"evaluation](../developer_guide/evaluation/using_ais_bench.md#execute-"
"performance-evaluation) for details."
msgstr ""
"详见 [使用 AISBench 进行性能评估](../developer_guide/evaluation/using_ais_bench.md"
"#execute-performance-evaluation)。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:294
msgid "Using vLLM Benchmark"
msgstr "使用 vLLM 基准测试"

#: ../../source/tutorials/Qwen3-235B-A22B.md:296
msgid "Run performance evaluation of `Qwen3-235B-A22B-w8a8` as an example."
msgstr "以运行 `Qwen3-235B-A22B-w8a8` 的性能评估为例。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:298
msgid ""
"Refer to [vllm "
"benchmark](https://docs.vllm.ai/en/latest/contributing/benchmarks.html) "
"for more details."
msgstr ""
"详见 [vLLM "
"基准测试文档](https://docs.vllm.ai/en/latest/contributing/benchmarks.html)。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:300
msgid "There are three `vllm bench` subcommand:"
msgstr "`vllm bench` 包含三个子命令："

#: ../../source/tutorials/Qwen3-235B-A22B.md:301
msgid "`latency`: Benchmark the latency of a single batch of requests."
msgstr "`latency`: 测试单批次请求的延迟。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:302
msgid "`serve`: Benchmark the online serving throughput."
msgstr "`serve`: 测试在线服务的吞吐量。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:303
msgid "`throughput`: Benchmark offline inference throughput."
msgstr "`throughput`: 测试离线推理的吞吐量。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:305
msgid "Take the `serve` as an example. Run the code as follows."
msgstr "以 `serve` 命令为例。运行代码如下。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:312
msgid ""
"After about several minutes, you can get the performance evaluation "
"result."
msgstr "大约几分钟后，即可获得性能评估结果。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:314
msgid "Reproducing Performance Results"
msgstr "复现性能结果"

#: ../../source/tutorials/Qwen3-235B-A22B.md:316
msgid ""
"In this section, we provide simple scripts to re-produce our latest "
"performance. It is also recommended to read instructions above to "
"understand basic concepts or options in vLLM && vLLM-Ascend."
msgstr "在本章节中，我们提供了复现最新性能数据的简易脚本。同时建议阅读上述指令以理解 vLLM 及 vLLM-Ascend 中的基本概念和选项。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:318
msgid "Environment"
msgstr "环境配置"

#: ../../source/tutorials/Qwen3-235B-A22B.md:320
msgid "vLLM v0.13.0"
msgstr "vLLM v0.13.0"

#: ../../source/tutorials/Qwen3-235B-A22B.md:321
msgid "vLLM-Ascend v0.13.0rc1"
msgstr "vLLM-Ascend v0.13.0rc1"

#: ../../source/tutorials/Qwen3-235B-A22B.md:322
msgid "CANN 8.3.RC2"
msgstr "CANN 8.3.RC2"

#: ../../source/tutorials/Qwen3-235B-A22B.md:323
msgid "torch_npu 2.8.0"
msgstr "torch_npu 2.8.0"

#: ../../source/tutorials/Qwen3-235B-A22B.md:324
msgid "HDK/driver 25.3.RC1"
msgstr "HDK/驱动 25.3.RC1"

#: ../../source/tutorials/Qwen3-235B-A22B.md:325
#, fuzzy
msgid "triton_ascend 3.2.0"
msgstr "triton_ascend 3.2.0.dev2025110717"

#: ../../source/tutorials/Qwen3-235B-A22B.md:327
msgid "Single Node A3 （64G*16）"
msgstr "单节点 A3 （64G*16）"

#: ../../source/tutorials/Qwen3-235B-A22B.md:329
msgid "Example server scripts:"
msgstr "服务端示例脚本："

#: ../../source/tutorials/Qwen3-235B-A22B.md:364
#: ../../source/tutorials/Qwen3-235B-A22B.md:599
msgid "Benchmark scripts:"
msgstr "基准测试脚本："

#: ../../source/tutorials/Qwen3-235B-A22B.md:380
#: ../../source/tutorials/Qwen3-235B-A22B.md:615
msgid "Reference test results:"
msgstr "参考测试结果："

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "num_requests"
msgstr "总请求数"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "concurrency"
msgstr "并发量"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "mean TTFT(ms)"
msgstr "平均 TTFT (ms)"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "mean TPOT(ms)"
msgstr "平均 TPOT (ms)"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "output token throughput (tok/s)"
msgstr "输出 Token 吞吐量 (tok/s)"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "720"
msgstr "720"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "144"
msgstr "144"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "4717.45"
msgstr "4717.45"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "48.69"
msgstr "48.69"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "2761.72"
msgstr "2761.72"

#: ../../source/tutorials/Qwen3-235B-A22B.md:386
#: ../../source/tutorials/Qwen3-235B-A22B.md:621
msgid "Note:"
msgstr "备注："

#: ../../source/tutorials/Qwen3-235B-A22B.md:387
msgid ""
"Setting `export VLLM_ASCEND_ENABLE_FUSED_MC2=1` enables MoE fused "
"operators that reduce time consumption of MoE in both prefill and decode."
" This is an experimental feature which only supports W8A8 quantization on"
" Atlas A3 servers now. If you encounter any problems when using this "
"feature, you can disable it by setting `export "
"VLLM_ASCEND_ENABLE_FUSED_MC2=0` and update issues in vLLM-Ascend "
"community."
msgstr ""
"设置 `export VLLM_ASCEND_ENABLE_FUSED_MC2=1` 将启用 MoE 融合算子，从而减少 MoE 在 "
"prefill 和 decode 阶段的耗时。这是一个实验性功能，目前仅支持 Atlas A3 服务器上的 W8A8 "
"量化。如果您在使用此功能时遇到任何问题，可以通过设置 `export VLLM_ASCEND_ENABLE_FUSED_MC2=0` "
"来禁用它，并在 vLLM-Ascend 社区反馈问题。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:388
msgid ""
"Here we disable prefix cache because of random datasets. You can enable "
"prefix cache if requests have long common prefix."
msgstr "由于数据集是随机的，我们在此禁用了前缀缓存。如果请求具有较长的公共前缀，您可以开启它。"

#: ../../source/tutorials/Qwen3-235B-A22B.md:390
msgid "Three Node A3 -- PD disaggregation"
msgstr "三节点 A3 -- 预填充-解码分离（PD 分离）"

#: ../../source/tutorials/Qwen3-235B-A22B.md:392
msgid ""
"On three Atlas 800 A3（64G*16）server, we recommend to use one node as one "
"prefill instance and two nodes as one decode instance. Example server "
"scripts: Prefill Node 1"
msgstr ""
"在三台 Atlas 800 A3（64G*16）服务器上，我们建议使用一个节点作为 prefill 实例，两个节点作为 decode "
"实例。服务端示例脚本：Prefill 节点 1"

#: ../../source/tutorials/Qwen3-235B-A22B.md:460
msgid "Decode Node 1"
msgstr "Decode 节点 1"

#: ../../source/tutorials/Qwen3-235B-A22B.md:526
msgid "Decode Node 2"
msgstr "Decode 节点 2"

#: ../../source/tutorials/Qwen3-235B-A22B.md:593
msgid "PD proxy:"
msgstr "PD 代理 (Proxy)："

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "2880"
msgstr "2880"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "576"
msgstr "576"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "3735.98"
msgstr "3735.98"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "52.07"
msgstr "52.07"

#: ../../source/tutorials/Qwen3-235B-A22B.md:76
msgid "8593.44"
msgstr "8593.44"

#: ../../source/tutorials/Qwen3-235B-A22B.md:622
msgid ""
"We recommend to set `export VLLM_ASCEND_ENABLE_FUSED_MC2=2` on this "
"scenario (typically EP32 for Qwen3-235B). This enables a different MoE "
"fusion operator."
msgstr ""
"在此场景下（通常 Qwen3-235B 为 EP32），我们建议设置 `export "
"VLLM_ASCEND_ENABLE_FUSED_MC2=2`。这将启用另一种不同的 MoE 融合算子。"

#~ msgid ""
#~ "**Notice:** triton_ascend is required for "
#~ "reproducing best performance of Qwen3-235B "
#~ "in vLLM-Ascend. If it is not "
#~ "installed in your environment, please "
#~ "follow the instructions below:"
#~ msgstr ""
#~ "**注意：** 复现 vLLM-Ascend 中 Qwen3-235B "
#~ "的最佳性能需要安装 triton_ascend。 如果您的环境中尚未安装，请参考以下说明进行安装："

