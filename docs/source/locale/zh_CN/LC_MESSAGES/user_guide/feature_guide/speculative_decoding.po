# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-21 10:23+0800\n"
"PO-Revision-Date: 2026-01-22 16:35+0800\n"
"Last-Translator: Gemini\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:1
msgid "Speculative Decoding Guide"
msgstr "推测解码"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:3
msgid ""
"This guide shows how to use Speculative Decoding with vLLM Ascend. "
"Speculative decoding is a technique which improves inter-token latency in"
" memory-bound LLM inference."
msgstr ""
"本指南介绍了如何在 vLLM Ascend 中使用猜测式解码（Speculative Decoding）。"
"猜测式解码是一种能够改善受内存带宽限制的 LLM 推理场景中 token 间延迟的技术。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:5
msgid "Speculating by matching n-grams in the prompt"
msgstr "通过匹配 Prompt 中的 n-gram 进行猜测"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:6
msgid ""
"The following code configures vLLM Ascend to use speculative decoding "
"where proposals are generated by matching n-grams in the prompt."
msgstr ""
"以下代码配置 vLLM Ascend 使用猜测式解码，其候选 token（proposals）是通过匹配 Prompt 中的 n-gram 生成的。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:8
#: ../../source/user_guide/feature_guide/speculative_decoding.md:41
#: ../../source/user_guide/feature_guide/speculative_decoding.md:121
msgid "Offline inference"
msgstr "离线推理"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:35
msgid "Speculating using EAGLE based draft models"
msgstr "使用基于 EAGLE 的草图模型进行猜测"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:37
msgid ""
"The following code configures vLLM Ascend to use speculative decoding "
"where proposals are generated by an [EAGLE (Extrapolation Algorithm for "
"Greater Language-model Efficiency)](https://arxiv.org/pdf/2401.15077) "
"based draft model."
msgstr ""
"以下代码配置 vLLM Ascend 使用猜测式解码，其候选 token 由基于 [EAGLE (Extrapolation Algorithm for "
"Greater Language-model Efficiency)](https://arxiv.org/pdf/2401.15077) 的草图模型（draft model）生成。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:39
msgid ""
"In v0.12.0rc1 of vLLM Ascend, the async scheduler is more stable and "
"ready to be enabled. We have adapted it to support EAGLE, and you can use"
" it by setting `async_scheduling=True` as follows. If you encounter any "
"issues, please feel free to open an issue on GitHub. As a workaround, you"
" can disable this feature by unsetting `async_scheduling=True` when "
"initializing the model."
msgstr ""
"在 vLLM Ascend v0.12.0rc1 中，异步调度器（async scheduler）更加稳定且已准备好启用。"
"我们已对其进行了适配以支持 EAGLE，您可以通过如下设置 `async_scheduling=True` 来使用它。"
"如果您遇到任何问题，请随时在 GitHub 上提交 issue。作为临时解决方案，您可以在初始化模型时取消设置 `async_scheduling=True` 来禁用此功能。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:73
msgid ""
"A few important things to consider when using the EAGLE based draft "
"models:"
msgstr "使用基于 EAGLE 的草图模型时需要注意的几点重要事项："

#: ../../source/user_guide/feature_guide/speculative_decoding.md:75
msgid ""
"The EAGLE draft models available in the [HF repository for EAGLE "
"models](https://huggingface.co/yuhuili) should be loaded and used "
"directly by vLLM. This functionality was added in PR "
"[#4893](https://github.com/vllm-project/vllm-ascend/pull/4893). If you "
"are using a vLLM version released before this pull request was merged, "
"please update to a more recent version."
msgstr ""
"[EAGLE 模型 HF 仓库](https://huggingface.co/yuhuili) 中的 EAGLE 草图模型应由 vLLM 直接加载和使用。"
"此功能已在 PR [#4893](https://github.com/vllm-project/vllm-ascend/pull/4893) 中添加。"
"如果您使用的 vLLM 版本早于该 PR 合并的时间，请更新到最新版本。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:79
msgid ""
"The EAGLE based draft models need to be run without tensor parallelism "
"(i.e. draft_tensor_parallel_size is set to 1 in `speculative_config`), "
"although it is possible to run the main model using tensor parallelism "
"(see example above)."
msgstr ""
"基于 EAGLE 的草图模型需要在不使用张量并行（Tensor Parallelism）的情况下运行（即 `speculative_config` 中的 `draft_tensor_parallel_size` 设置为 1），"
"尽管主模型可以使用张量并行运行（见上方示例）。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:83
msgid ""
"When using EAGLE-3 based draft model, option \"method\" must be set to "
"\"eagle3\". That is, to specify `\"method\": \"eagle3\"` in "
"`speculative_config`."
msgstr ""
"当使用基于 EAGLE-3 的草图模型时，选项 \"method\" 必须设置为 \"eagle3\"。即在 `speculative_config` 中指定 `\"method\": \"eagle3\"`。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:86
msgid "Speculating using MTP speculators"
msgstr "使用 MTP 预测器进行猜测"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:88
msgid ""
"The following code configures vLLM Ascend to use speculative decoding "
"where proposals are generated by MTP (Multi Token Prediction), boosting "
"inference performance by parallelizing the prediction of multiple tokens."
" For more information about MTP see "
"[Multi_Token_Prediction](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/feature_guide/Multi_Token_Prediction.html)"
msgstr ""
"以下代码配置 vLLM Ascend 使用猜测式解码，其候选 token 由 MTP (Multi Token Prediction) 生成。"
"通过并行化多个 token 的预测，MTP 可以提升推理性能。有关 MTP 的更多信息，请参阅 "
"[Multi_Token_Prediction](https://docs.vllm.ai/projects/ascend/en/latest/developer_guide/feature_guide/Multi_Token_Prediction.html)"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:90
msgid "Online inference"
msgstr "在线推理"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:110
msgid "Speculating using Suffix Decoding"
msgstr "使用 Suffix Decoding 进行猜测"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:112
msgid ""
"The following code configures vLLM to use speculative decoding where "
"proposals are generated using Suffix Decoding [(SuffixDecoding: Extreme "
"Speculative Decoding for Emerging AI "
"Applications)](https://arxiv.org/abs/2411.04975)."
msgstr ""
"以下代码配置 vLLM 使用猜测式解码，其候选 token 使用 Suffix Decoding 生成 [(SuffixDecoding: Extreme "
"Speculative Decoding for Emerging AI Applications)](https://arxiv.org/abs/2411.04975)。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:114
msgid ""
"Like n-gram, Suffix Decoding can generate draft tokens by pattern-"
"matching using the last `n` generated tokens. Unlike n-gram, Suffix "
"Decoding (1) can pattern-match against both the prompt and previous "
"generations, (2) uses frequency counts to propose the most likely "
"continuations, and (3) speculates an adaptive number of tokens for each "
"request at each iteration to get better acceptance rates."
msgstr ""
"与 n-gram 类似，Suffix Decoding 可以通过匹配最后 `n` 个生成的 token 进行模式匹配来生成草图 token。"
"与 n-gram 不同的是，Suffix Decoding：(1) 可以同时针对 Prompt 和之前的生成内容进行模式匹配；"
"(2) 使用频率计数来提议最可能的后续内容；(3) 为每个请求在每次迭代中猜测自适应数量的 token，以获得更好的接受率。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:116
msgid ""
"Suffix Decoding can achieve better performance for tasks with high "
"repetition, such as code-editing, agentic loops (e.g. self-reflection, "
"self-consistency), and RL rollouts."
msgstr ""
"Suffix Decoding 在高重复性任务中可以获得更好的性能，例如代码编辑、智能体循环（如自我反思、自洽性）以及强化学习（RL）的 rollout 过程。"

#: ../../source/user_guide/feature_guide/speculative_decoding.md:118
msgid ""
"[!NOTE] Suffix Decoding requires Arctic Inference. You can install it "
"with `pip install arctic-inference`."
msgstr ""
"[!注意] Suffix Decoding 需要使用 Arctic Inference。您可以通过 `pip install arctic-inference` 进行安装。"