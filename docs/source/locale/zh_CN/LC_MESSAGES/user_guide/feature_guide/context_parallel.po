# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-21 10:23+0800\n"
"PO-Revision-Date: 2026-01-22 18:20+0800\n"
"Last-Translator: Gemini\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/user_guide/feature_guide/context_parallel.md:1
msgid "Context Parallel Guide"
msgstr "上下文并行"

#: ../../source/user_guide/feature_guide/context_parallel.md:3
msgid "Overview"
msgstr "概述"

#: ../../source/user_guide/feature_guide/context_parallel.md:5
msgid ""
"This guide shows how to use Context Parallel, a long sequence inference "
"optimization technique. Context Parallel includes `PCP` (Prefill Context "
"Parallel) and `DCP` (Decode Context Parallel), which reduces NPU memory "
"usage and improves inference speed in long sequence LLM inference."
msgstr ""
"本指南介绍如何使用上下文并行（Context Parallel），这是一种长序列推理优化技术。"
"上下文并行包括 `PCP`（预填充上下文并行）和 `DCP`（解码上下文并行），能够降低长序列 LLM 推理中的 NPU 显存占用并提高推理速度。"

#: ../../source/user_guide/feature_guide/context_parallel.md:7
msgid "Benefits of Context Parallel"
msgstr "上下文并行的优势"

#: ../../source/user_guide/feature_guide/context_parallel.md:8
msgid ""
"Context parallel mainly solves the problem of serving long context "
"requests. As prefill and decode present quite different characteristics "
"and have quite different SLO (service level objectives), we need to "
"implement context parallel separately for them. The major considerations "
"are:"
msgstr ""
"上下文并行主要解决长上下文请求的服务问题。由于预填充（Prefill）和解码（Decode）呈现出截然不同的特性，"
"且具有不同的 SLO（服务水平目标），我们需要分别为它们实现上下文并行。主要考量点包括："

#: ../../source/user_guide/feature_guide/context_parallel.md:10
msgid ""
"For long context prefill, we can use context parallel to reduce TTFT "
"(time to first token) by amortizing the computation time of the prefill "
"across query tokens."
msgstr ""
"对于长上下文预填充，我们可以使用上下文并行通过在查询 Token 之间分摊预填充的计算时间，从而降低 TTFT（首字延迟）。"

#: ../../source/user_guide/feature_guide/context_parallel.md:11
msgid ""
"For long context decode, we can use context parallel to reduce KV cache "
"duplication and offer more space for KV cache to increase the batchsize "
"(and hence the throughput)."
msgstr ""
"对于长上下文解码，我们可以使用上下文并行来减少 KV Cache 的重复，并为 KV Cache 提供更多空间以增加 Batch Size（从而提高吞吐量）。"

#: ../../source/user_guide/feature_guide/context_parallel.md:13
msgid ""
"To learn more about the theory and implementation details of context "
"parallel, please refer to the [context parallel developer "
"guide](../../developer_guide/feature_guide/context_parallel.md)."
msgstr ""
"欲了解更多关于上下文并行的理论和实现细节，请参考 [上下文并行开发者指南](../../developer_guide/feature_guide/context_parallel.md)。"

#: ../../source/user_guide/feature_guide/context_parallel.md:15
msgid "Supported Scenarios"
msgstr "支持场景"

#: ../../source/user_guide/feature_guide/context_parallel.md:16
msgid ""
"Currently context parallel can be used together with most other features,"
" supported features are as follows:"
msgstr "目前上下文并行可以与大多数其他功能协同使用，支持的功能如下："

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Eager"
msgstr "Eager 模式"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Graph"
msgstr "图模式 (Graph)"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Prefix <br> Cache"
msgstr "前缀缓存 <br> (Prefix Cache)"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Chunked <br> Prefill"
msgstr "分块预填充 <br> (Chunked Prefill)"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "SpecDecode <br> (MTP)"
msgstr "投机解码 <br> (MTP)"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "PD <br> disaggregation"
msgstr "PD 分离 <br> (PD disaggregation)"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "MLAPO"
msgstr "MLAPO"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "**PCP**"
msgstr "**PCP**"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "✅"
msgstr "✅"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "**DCP**"
msgstr "**DCP**"

#: ../../source/user_guide/feature_guide/context_parallel.md:22
msgid "How to use Context Parallel"
msgstr "如何使用上下文并行"

#: ../../source/user_guide/feature_guide/context_parallel.md:23
msgid ""
"You can enable `PCP` and `DCP` by `prefill_context_parallel_size` and "
"`decode_context_parallel_size`, refer to the following example:"
msgstr "您可以通过 `prefill_context_parallel_size` 和 `decode_context_parallel_size` 启用 `PCP` 和 `DCP`，参考如下示例："

#: ../../source/user_guide/feature_guide/context_parallel.md:25
msgid "Offline example:"
msgstr "离线示例："

#: ../../source/user_guide/feature_guide/context_parallel.md:44
msgid "Online example:"
msgstr "在线示例："

#: ../../source/user_guide/feature_guide/context_parallel.md:53
msgid ""
"The total world_size is `tensor_parallel_size` * "
"`prefill_context_parallel_size`, so the examples above need 4 NPUs for "
"each."
msgstr "总的 world_size 是 `tensor_parallel_size` * `prefill_context_parallel_size`，因此上述示例每个都需要 4 个 NPU。"

#: ../../source/user_guide/feature_guide/context_parallel.md:55
msgid "Constraints"
msgstr "限制条件"

#: ../../source/user_guide/feature_guide/context_parallel.md:56
msgid "While using DCP, the following constraints must be met:"
msgstr "使用 DCP 时，必须满足以下限制条件："

#: ../../source/user_guide/feature_guide/context_parallel.md:57
msgid "For MLA based model, such as Deepseek-R1:"
msgstr "对于基于 MLA 的模型，如 DeepSeek-R1："

#: ../../source/user_guide/feature_guide/context_parallel.md:58
msgid "`tensor_parallel_size >= decode_context_parallel_size`"
msgstr "`tensor_parallel_size >= decode_context_parallel_size`"

#: ../../source/user_guide/feature_guide/context_parallel.md:59
#, python-format
msgid "`tensor_parallel_size % decode_context_parallel_size == 0`"
msgstr "`tensor_parallel_size % decode_context_parallel_size == 0`"

#: ../../source/user_guide/feature_guide/context_parallel.md:60
msgid "For GQA based model, such as Qwen3-235B:"
msgstr "对于基于 GQA 的模型，如 Qwen3-235B："

#: ../../source/user_guide/feature_guide/context_parallel.md:61
msgid ""
"`(tensor_parallel_size // num_key_value_heads) >= "
"decode_context_parallel_size`"
msgstr "`(tensor_parallel_size // num_key_value_heads) >= decode_context_parallel_size`"

#: ../../source/user_guide/feature_guide/context_parallel.md:62
#, python-format
msgid ""
"`(tensor_parallel_size // num_key_value_heads) % "
"decode_context_parallel_size == 0`"
msgstr "`(tensor_parallel_size // num_key_value_heads) % decode_context_parallel_size == 0`"

#: ../../source/user_guide/feature_guide/context_parallel.md:64
msgid ""
"While using Context Parallel in KV cache transfer needed scenario (e.g. "
"KV pooling, PD-disaggregation), to simplify KV cache transmission, "
"`cp_kv_cache_interleave_size` must be set to the same value of KV cache "
"`block_size`(default: 128), which specify cp to split KV cache in a "
"block-interleave style. For example:"
msgstr ""
"在需要进行 KV Cache 传输的场景（如 KV Pooling、PD 分离）中使用上下文并行时，为了简化 KV Cache 传输，"
"必须将 `cp_kv_cache_interleave_size` 设置为与 KV Cache `block_size`（默认值：128）相同的值，"
"这将指定上下文并行以块交织（block-interleave）风格切分 KV Cache。例如："

#: ../../source/user_guide/feature_guide/context_parallel.md:75
msgid "Experimental Results"
msgstr "实验结果"

#: ../../source/user_guide/feature_guide/context_parallel.md:76
msgid ""
"To evaluate the effectiveness of Context Parallel in in long sequence LLM"
" inference scenarios, we use **DeepSeek-R1-W8A8** and **Qwen3-235B**, "
"deploy PD-disaggregate instances in the environment of 64 cards Ascend "
"910C*64G (A3), the configuration and performance data are as follows."
msgstr ""
"为了评估上下文并行在长序列 LLM 推理场景中的有效性，我们使用了 **DeepSeek-R1-W8A8** 和 **Qwen3-235B** 模型，"
"并在 64 卡 Ascend 910C*64G (A3) 环境下部署 PD 分离实例，配置和性能数据如下。"

#: ../../source/user_guide/feature_guide/context_parallel.md:78
msgid "DeepSeek-R1-W8A8:"
msgstr "DeepSeek-R1-W8A8："

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Configuration"
msgstr "配置"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Input length <br> 32k"
msgstr "输入长度 <br> 32k"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Input length <br> 64k"
msgstr "输入长度 <br> 64k"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Input length <br> 128k"
msgstr "输入长度 <br> 128k"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "P node: (DP2 TP8 EP16) *2 <br> D node: (DP32 EP32) *1"
msgstr "P 节点: (DP2 TP8 EP16) *2 <br> D 节点: (DP32 EP32) *1"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 9.3s <br> TPOT: 72ms"
msgstr "TTFT: 9.3s <br> TPOT: 72ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 22.8s <br> TPOT: 74ms"
msgstr "TTFT: 22.8s <br> TPOT: 74ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 73.2s <br> TPOT: 82ms"
msgstr "TTFT: 73.2s <br> TPOT: 82ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "P node: (PCP2 TP8 DCP8 EP16) *2 <br> D node: (DP32 EP32) *1"
msgstr "P 节点: (PCP2 TP8 DCP8 EP16) *2 <br> D 节点: (DP32 EP32) *1"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 7.9s <br> TPOT: 74ms"
msgstr "TTFT: 7.9s <br> TPOT: 74ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 15.9s <br> TPOT: 78ms"
msgstr "TTFT: 15.9s <br> TPOT: 78ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 46.0s <br> TPOT: 83ms"
msgstr "TTFT: 46.0s <br> TPOT: 83ms"

#: ../../source/user_guide/feature_guide/context_parallel.md:84
msgid "Qwen3-235B:"
msgstr "Qwen3-235B："

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "Input length <br> 120k"
msgstr "输入长度 <br> 120k"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 5.1s <br> TPOT: 65ms"
msgstr "TTFT: 5.1s <br> TPOT: 65ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 13.1s <br> TPOT: 85ms"
msgstr "TTFT: 13.1s <br> TPOT: 85ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 33.9s <br> TPOT: 120ms"
msgstr "TTFT: 33.9s <br> TPOT: 120ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "P node: (PCP2 TP8 DCP2 EP16) *2 <br> D node: (DP32 EP32) *1"
msgstr "P 节点: (PCP2 TP8 DCP2 EP16) *2 <br> D 节点: (DP32 EP32) *1"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 3.0s <br> TPOT: 66ms"
msgstr "TTFT: 3.0s <br> TPOT: 66ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 8.9s <br> TPOT: 86ms"
msgstr "TTFT: 8.9s <br> TPOT: 86ms"

#: ../../source/user_guide/feature_guide/context_parallel.md
msgid "TTFT: 22.7s <br> TPOT: 121ms"
msgstr "TTFT: 22.7s <br> TPOT: 121ms"