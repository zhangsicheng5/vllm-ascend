# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version:  vllm-ascend\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-21 10:23+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/community/user_stories/llamafactory.md:1
msgid "LLaMA-Factory"
msgstr "LLaMA-Factory"

#: ../../source/community/user_stories/llamafactory.md:3
msgid "**Introduction**"
msgstr "**简介**"

#: ../../source/community/user_stories/llamafactory.md:5
msgid ""
"[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) is an easy-to-"
"use and efficient platform for training and fine-tuning large language "
"models. With LLaMA-Factory, you can fine-tune hundreds of pre-trained "
"models locally without writing any code."
msgstr ""
"[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) 是一个易于使用且高效的大语言模型训练与微调平台。通过 LLaMA-Factory，您可以在本地对数百个预训练模型进行微调，无需编写任何代码。"

#: ../../source/community/user_stories/llamafactory.md:7
msgid ""
"LLaMA-Facotory users need to evaluate and inference the model after fine-"
"tuning."
msgstr "LLaMA-Factory 用户在模型微调后，需要对其进行评估和推理。"

#: ../../source/community/user_stories/llamafactory.md:9
msgid "**Business challenge**"
msgstr "**面临的问题**"

#: ../../source/community/user_stories/llamafactory.md:11
msgid ""
"LLaMA-Factory uses Transformers to perform inference on Ascend NPUs, but "
"the speed is slow."
msgstr "LLaMA-Factory 最初使用 Transformers 在昇腾 NPU 上进行推理，但推理速度较慢。"

#: ../../source/community/user_stories/llamafactory.md:13
msgid "**Benefits with vLLM Ascend**"
msgstr "**使用 vLLM Ascend 的优势**"

#: ../../source/community/user_stories/llamafactory.md:15
msgid ""
"With the joint efforts of LLaMA-Factory and vLLM Ascend ([LLaMA-"
"Factory#7739](https://github.com/hiyouga/LLaMA-Factory/pull/7739)), "
"LLaMA-Factory has achieved significant performance gains during model "
"inference. Benchmark results show that its inference speed is now up to "
"2× faster compared to the Transformers implementation."
msgstr "通过 LLaMA-Factory 与 vLLM Ascend 的共同努力（[LLaMA-Factory#7739](https://github.com/hiyouga/LLaMA-Factory/pull/7739)），LLaMA-Factory 在模型推理阶段取得了显著的性能提升。基准测试结果表明，其推理速度相比原有的 Transformers 实现提升高达 2 倍。

#: ../../source/community/user_stories/llamafactory.md:17
msgid "**Learn more**"
msgstr "**了解更多**"

#: ../../source/community/user_stories/llamafactory.md:19
msgid ""
"See more details about LLaMA-Factory and how it uses vLLM Ascend for "
"inference on Ascend NPUs in [LLaMA-Factory Ascend NPU "
"Inference](https://llamafactory.readthedocs.io/en/latest/advanced/npu_inference.html)."
msgstr "关于 LLaMA-Factory 的更多详细信息，以及它如何在昇腾 NPU 上使用 vLLM Ascend 进行推理，请参阅 [LLaMA-Factory 昇腾 NPU 推理](https://llamafactory.readthedocs.io/en/latest/advanced/npu_inference.html) 文档。"

#~ msgid ""
#~ "With the joint efforts of LLaMA-"
#~ "Factory and vLLM Ascend ([LLaMA-"
#~ "Factory#7739](https://github.com/hiyouga/LLaMA-"
#~ "Factory/pull/7739)), the performance of "
#~ "LLaMA-Factory in the model inference "
#~ "stage has been significantly improved. "
#~ "According to the test results, the "
#~ "inference speed of LLaMA-Factory has "
#~ "been increased to 2x compared to "
#~ "the transformers version."
#~ msgstr ""
#~ "在 LLaMA-Factory 和 vLLM Ascend "
#~ "的共同努力下（参见 [LLaMA-Factory#7739](https://github.com/hiyouga"
#~ "/LLaMA-Factory/pull/7739)），LLaMA-Factory "
#~ "在模型推理阶段的性能得到了显著提升。根据测试结果，LLaMA-Factory 的推理速度相比 "
#~ "transformers 版本提升到了 2 倍。"

