# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2026.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: vllm-ascend \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-21 10:23+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/feature_guide/quantization.md:1
msgid "Quantization Adaptation Guide"
msgstr "量化适配"

#: ../../source/developer_guide/feature_guide/quantization.md:3
msgid ""
"This document provides guidance for adapting quantization algorithms and "
"models related to **ModelSlim**."
msgstr "本文档提供了与 **ModelSlim** 相关的量化算法和模型适配指南。"

#: ../../source/developer_guide/feature_guide/quantization.md:5
msgid "Quantization Feature Introduction"
msgstr "量化功能介绍"

#: ../../source/developer_guide/feature_guide/quantization.md:7
msgid "Quantization Inference Process"
msgstr "量化推理流程"

#: ../../source/developer_guide/feature_guide/quantization.md:9
msgid ""
"The current process for registering and obtaining quantization methods in"
" vLLM Ascend is as follows:"
msgstr "当前 vLLM Ascend 中注册并获取量化方法的过程如下："

#: ../../source/developer_guide/feature_guide/quantization.md:11
msgid "![get_quant_method](../../assets/quantization/get_quant_method.png)"
msgstr "![获取量化方法](../../assets/quantization/get_quant_method.png)"

#: ../../source/developer_guide/feature_guide/quantization.md:11
msgid "get_quant_method"
msgstr "获取量化方法"

#: ../../source/developer_guide/feature_guide/quantization.md:13
msgid ""
"vLLM Ascend registers a custom ascend quantization method. By configuring"
" the `--quantization ascend` parameter (or `quantization=\"ascend\"` for "
"offline), the quantization feature is enabled. When constructing the "
"`quant_config`, the registered `AscendQuantConfig` is initialized and "
"`get_quant_method` is called to obtain the quantization method "
"corresponding to each weight part, stored in the `quant_method` "
"attribute."
msgstr "vLLM Ascend 注册了自定义的 ascend 量化方法。通过配置 `--quantization ascend` 参数（或离线模式下的 `quantization=\"ascend\"`）来启用量化功能。构建 `quant_config` 时，会初始化已注册的 `AscendQuantConfig`，并调用 `get_quant_method` 来获取与每个权重部分对应的量化方法，存储在 `quant_method` 属性中。"

#: ../../source/developer_guide/feature_guide/quantization.md:15
msgid ""
"Currently supported quantization methods include `AscendLinearMethod`, "
"`AscendFusedMoEMethod`, `AscendEmbeddingMethod`, and their corresponding "
"non-quantized methods:"
msgstr "目前支持的量化方法包括 `AscendLinearMethod`、`AscendFusedMoEMethod`、`AscendEmbeddingMethod` 及其对应的非量化方法："

#: ../../source/developer_guide/feature_guide/quantization.md:17
msgid "![quant_methods_overview](../../assets/quantization/quant_methods_overview.png)"
msgstr "![量化方法概览](../../assets/quantization/quant_methods_overview.png)"

#: ../../source/developer_guide/feature_guide/quantization.md:17
msgid "quant_methods_overview"
msgstr "量化方法概览"

#: ../../source/developer_guide/feature_guide/quantization.md:19
msgid ""
"The quantization method base class defined by vLLM  and the overall call "
"flow of quantization methods are as follows:"
msgstr "vLLM 定义的量化方法基类及量化方法的整体调用流程如下："

#: ../../source/developer_guide/feature_guide/quantization.md:21
msgid "![quant_method_call_flow](../../assets/quantization/quant_method_call_flow.png)"
msgstr "![量化方法调用流程](../../assets/quantization/quant_method_call_flow.png)"

#: ../../source/developer_guide/feature_guide/quantization.md:21
msgid "quant_method_call_flow"
msgstr "量化方法调用流程"

#: ../../source/developer_guide/feature_guide/quantization.md:23
msgid ""
"The `embedding` method is generally not implemented for quantization, "
"focusing only on the other three methods."
msgstr "`embedding` 方法通常不实现量化，重点在于其他三种方法。"

#: ../../source/developer_guide/feature_guide/quantization.md:25
msgid ""
"The `create_weights` method is used for weight initialization; the "
"`process_weights_after_loading` method is used for weight post-"
"processing, such as transposition, format conversion, data type "
"conversion, etc.; the `apply` method is used to perform activation "
"quantization and quantized matrix multiplication calculations during the "
"forward process."
msgstr "`create_weights` 方法用于权重初始化；`process_weights_after_loading` 方法用于权重后处理，例如转置、格式转换、数据类型转换等；`apply` 方法用于在前向过程中执行激活量化和量化矩阵乘法计算。"

#: ../../source/developer_guide/feature_guide/quantization.md:27
msgid ""
"We need to implement the `create_weights`, "
"`process_weights_after_loading`, and `apply` methods for different "
"**layers** (**attention**, **mlp**, **moe**)."
msgstr "我们需要为不同的**层**（**attention**、**mlp**、**moe**）实现 `create_weights`、`process_weights_after_loading` 和 `apply` 方法。"

#: ../../source/developer_guide/feature_guide/quantization.md:29
msgid ""
"**Supplemnet**: When loading the model, the quantized model's description"
" file **quant_model_description.json** needs to be read. This file "
"describes the quantization configuration and parameters for each part of "
"the model weights, for example:"
msgstr "**补充说明**：加载模型时，需要读取量化模型的描述文件 **quant_model_description.json**。该文件描述了模型权重各个部分的量化配置和参数，例如："

#: ../../source/developer_guide/feature_guide/quantization.md:49
msgid ""
"Based on the above content, we present a brief description of the "
"adaptation process for quantization algorithms and quantized models."
msgstr "基于以上内容，我们对量化算法和量化模型的适配过程进行简要描述。"

#: ../../source/developer_guide/feature_guide/quantization.md:51
msgid "Quantization Algorithm Adaptation"
msgstr "量化算法适配"

#: ../../source/developer_guide/feature_guide/quantization.md:53
msgid ""
"**Step 1: Algorithm Design**. Define the algorithm ID (e.g., "
"`W4A8_DYNAMIC`), determine supported layers (linear, moe, attention), and"
" design the quantization scheme (static/dynamic, "
"pertensor/perchannel/pergroup)."
msgstr "**步骤 1：算法设计**。定义算法 ID（例如 `W4A8_DYNAMIC`），确定支持的层（线性层、moe、attention），并设计量化方案（静态/动态、按张量/按通道/按分组）。"

#: ../../source/developer_guide/feature_guide/quantization.md:54
msgid ""
"**Step 2: Registration**. Add the algorithm ID to "
"`ASCEND_QUANTIZATION_METHOD_MAP` in `vllm_ascend/quantization/utils.py` "
"and associate it with the corresponding method class."
msgstr "**步骤 2：注册**。将算法 ID 添加到 `vllm_ascend/quantization/utils.py` 中的 `ASCEND_QUANTIZATION_METHOD_MAP`，并将其与对应的方法类关联。"

#: ../../source/developer_guide/feature_guide/quantization.md:65
msgid ""
"**Step 3: Implementation**. Create an algorithm implementation file, such"
" as `vllm_ascend/quantization/w4a8_dynamic.py`, and implement the method "
"class and logic."
msgstr "**步骤 3：实现**。创建算法实现文件，例如 `vllm_ascend/quantization/w4a8_dynamic.py`，并实现方法类及其逻辑。"

#: ../../source/developer_guide/feature_guide/quantization.md:66
msgid ""
"**Step 4: Testing**. Use your algorithm to generate quantization "
"configurations and verify correctness and performance on target models "
"and hardware."
msgstr "**步骤 4：测试**。使用您的算法生成量化配置，并在目标模型和硬件上验证正确性与性能。"

#: ../../source/developer_guide/feature_guide/quantization.md:68
msgid "Quantized Model Adaptation"
msgstr "量化模型适配"

#: ../../source/developer_guide/feature_guide/quantization.md:70
msgid ""
"Adapting a new quantized model requires ensuring the following three "
"points:"
msgstr "适配新的量化模型需要确保以下三点："

#: ../../source/developer_guide/feature_guide/quantization.md:72
msgid "The original model has been successfully adapted in `vLLM Ascend`."
msgstr "原始模型已在 `vLLM Ascend` 中成功适配。"

#: ../../source/developer_guide/feature_guide/quantization.md:73
msgid ""
"**Fused Module Mapping**: Add the model's `model_type` to "
"`packed_modules_model_mapping` in "
"`vllm_ascend/quantization/quant_config.py` (e.g., `qkv_proj`, "
"`gate_up_proj`, `experts`) to ensure sharding consistency and correct "
"loading."
msgstr "**融合模块映射**：将模型的 `model_type` 添加到 `vllm_ascend/quantization/quant_config.py` 中的 `packed_modules_model_mapping`（例如 `qkv_proj`、`gate_up_proj`、`experts`），以确保分片一致性和正确加载。"

#: ../../source/developer_guide/feature_guide/quantization.md:93
msgid ""
"All quantization algorithms used by the quantized model have been "
"integrated into the `quantization` module."
msgstr "量化模型使用的所有量化算法都已集成到 `quantization` 模块中。"

#: ../../source/developer_guide/feature_guide/quantization.md:95
msgid "Currently Supported Quantization Algorithms"
msgstr "当前支持的量化算法"

#: ../../source/developer_guide/feature_guide/quantization.md:97
msgid ""
"vLLM Ascend supports multiple quantization algorithms. The following "
"table provides an overview of each quantization algorithm based on the "
"implementation in the `vllm_ascend.quantization` module:"
msgstr "vLLM Ascend 支持多种量化算法。下表基于 `vllm_ascend.quantization` 模块中的实现，概述了每种量化算法："

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Algorithm"
msgstr "算法"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Weight"
msgstr "权重"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Activation"
msgstr "激活"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Weight Granularity"
msgstr "权重粒度"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Activation Granularity"
msgstr "激活粒度"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Type"
msgstr "类型"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Description"
msgstr "描述"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W4A16`"
msgstr "`W4A16`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "INT4"
msgstr "INT4"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "FP16/BF16"
msgstr "FP16/BF16"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Group"
msgstr "按分组"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Tensor"
msgstr "按张量"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Static"
msgstr "静态"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"4-bit weight quantization with 16-bit activation precision, specifically "
"designed for MoE model expert layers, supporting int32 format weight "
"packing"
msgstr "4 位权重量化，16 位激活精度，专为 MoE 模型专家层设计，支持 int32 格式的权重打包"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A16`"
msgstr "`W8A16`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "INT8"
msgstr "INT8"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Channel"
msgstr "按通道"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"8-bit weight quantization with 16-bit activation precision, balancing "
"accuracy and performance, suitable for linear layers"
msgstr "8 位权重量化，16 位激活精度，在精度和性能间取得平衡，适用于线性层"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A8`"
msgstr "`W8A8`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"Static activation quantization, suitable for scenarios requiring high "
"precision"
msgstr "静态激活量化，适用于需要高精度的场景"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A8_DYNAMIC`"
msgstr "`W8A8_DYNAMIC`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Token"
msgstr "按 token"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Dynamic"
msgstr "动态"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Dynamic activation quantization with per-token scaling factor calculation"
msgstr "动态激活量化，按 token 计算缩放因子"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W4A8_DYNAMIC`"
msgstr "`W4A8_DYNAMIC`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"Supports both direct per-channel quantization to 4-bit and two-step "
"quantization (per-channel to 8-bit then per-group to 4-bit)"
msgstr "支持直接按通道量化到 4 位，以及两步量化（先按通道量化到 8 位，再按分组量化到 4 位）"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W4A4_FLATQUANT_DYNAMIC`"
msgstr "`W4A4_FLATQUANT_DYNAMIC`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"Uses FlatQuant for activation distribution smoothing before 4-bit dynamic"
" quantization, with additional matrix multiplications for precision "
"preservation"
msgstr "在 4 位动态量化前使用 FlatQuant 对激活分布进行平滑处理，并通过额外的矩阵乘法来保持精度"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "`W8A8_MIX`"
msgstr "`W8A8_MIX`"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Per-Tensor/Token"
msgstr "按张量/token"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid "Mixed"
msgstr "混合"

#: ../../source/developer_guide/feature_guide/quantization.md
msgid ""
"PD Colocation Scenario uses dynamic quantization for both P node and D "
"node; PD Disaggregation Scenario uses dynamic quantization for P node and"
" static for D node"
msgstr "PD 共置场景下，P 节点和 D 节点均使用动态量化；PD 分离场景下，P 节点使用动态量化，D 节点使用静态量化"

#: ../../source/developer_guide/feature_guide/quantization.md:109
msgid ""
"**Static vs Dynamic:** Static quantization uses pre-computed scaling "
"factors with better performance, while dynamic quantization computes "
"scaling factors on-the-fly for each token/activation tensor with higher "
"precision."
msgstr "**静态与动态：** 静态量化使用预计算的缩放因子，性能更佳；动态量化则为每个 token/激活张量实时计算缩放因子，精度更高。"

#: ../../source/developer_guide/feature_guide/quantization.md:111
msgid ""
"**Granularity:** Refers to the scope of scaling factor computation (e.g.,"
" per-tensor, per-channel, per-group)."
msgstr "**粒度：** 指缩放因子计算的范围（例如，按张量、按通道、按分组）。"