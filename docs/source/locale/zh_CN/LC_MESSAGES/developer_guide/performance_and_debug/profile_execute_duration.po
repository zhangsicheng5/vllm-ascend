# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2025, vllm-ascend team
# This file is distributed under the same license as the vllm-ascend
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2025.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version:  vllm-ascend\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2026-01-21 10:23+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: zh_CN\n"
"Language-Team: zh_CN <LL@li.org>\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.17.0\n"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:1
msgid "Profile Execute Duration"
msgstr "执行时长分析"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:3
msgid ""
"The execution duration of each stage (including pre/post-processing, "
"model forward, etc.) usually needs to be captured during a complete "
"inference process. Typically, this is done by using "
"`torch.npu.synchronize()` and obtaining CPU timestamps, which increases "
"the performance overhead of host/device synchronization."
msgstr ""
"在完整的推理过程中，通常需要记录每个阶段（包括前/后处理、模型前向等）的执行时长。传统方法通常通过调用 "
"`torch.npu.synchronize()` 并获取 CPU 时间戳来实现，这会增加主机/设备同步的性能开销。"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:5
msgid ""
"**To reduce the performance overhead, we add this feature, using the NPU "
"event timestamp mechanism to observe the device execution time "
"asynchronously.**"
msgstr "**为降低性能开销，我们新增了此功能，利用 NPU 事件时间戳机制异步观测设备执行时间。**"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:7
msgid "Usage"
msgstr "使用方法"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:8
msgid ""
"Use the environment variable `VLLM_ASCEND_MODEL_EXECUTE_TIME_OBSERVE` to "
"enable this feature."
msgstr "使用环境变量 `VLLM_ASCEND_MODEL_EXECUTE_TIME_OBSERVE` 来启用此功能。"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:9
msgid ""
"Use the non-blocking API `ProfileExecuteDuration().capture_async` to set "
"observation points asynchronously when you need to observe the execution "
"duration."
msgstr "当需要观测执行时长时，使用非阻塞 API `ProfileExecuteDuration().capture_async` 异步设置观测点。"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:10
msgid ""
"Use the blocking API `ProfileExecuteDuration().pop_captured_sync` at an "
"appropriate time to get and print the execution durations of all observed"
" stages."
msgstr "在适当时机使用阻塞 API `ProfileExecuteDuration().pop_captured_sync` 获取并打印所有已观测阶段的执行时长。"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:12
msgid ""
"**We have instrumented the key inference stages (including pre-"
"processing, model forward pass, etc.) for execution duration profiling. "
"Execute the script as follows:**"
msgstr "**我们已对关键的推理阶段（包括预处理、模型前向传播等）进行了执行时长分析的插桩。请按如下方式执行脚本：**"

#: ../../source/developer_guide/performance_and_debug/profile_execute_duration.md:18
msgid "Example Output"
msgstr "示例输出"